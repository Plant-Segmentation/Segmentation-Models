README

1. Prepare dataset 

Preprocess the Dataset to convert them into TFRecords
a) Split the dataset into classes
    - train.txt
    - trainval.txt
    - val.txt (ignoring this for now)
b) generate TFRecords
    refer this - https://github.com/tensorflow/models/blob/master/research/deeplab/datasets/download_and_convert_voc2012.sh
    
    OR 
    
    cd datasets/

    run the command -
    python ./build_plant_data.py \
  --image_folder="./Plant-data/JPEG" \
  --semantic_segmentation_folder="./Plant-data/AnnotationResult/category4/labels_gray" \
  --list_folder="./Plant-data/AnnotationResult/category4/Segmentation-labels/" \
  --image_format="jpg" \
  --output_dir="./Plant-data/tfrecord"

  ** No of training examples - 102 (80% of 127)
  ** No of trainingval examples - 25 (20% of 127)

  2. Add the dataset information in the data_generator file.
    
    datasets/data_generator.py

    _PLANT_CATEGORY4_INFORMATION = DatasetDescriptor(
        splits_to_sizes={
            'train': 102,  # num of samples in images/training
            'trainval': 25,  # num of samples in images/validation
            'test': 127,
        },
        num_classes=4,
        ignore_label=255,
    )

    _DATASETS_INFORMATION = {
        'cityscapes': _CITYSCAPES_INFORMATION,
        'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
        'ade20k': _ADE20K_INFORMATION,
        'category4': _PLANT_CATEGORY4_INFORMATION,
    }

  3. Modify the Colormap Configuration for the dataset:
     utils/get_dataset_colormap.py

    Note: Name of dataset == _CATEGORY4
    use this function - label_to_color_image(label, dataset=_CATEGORY4)

  4. Since we have imbalanced dataset, modify train_utils.py to assign weights for different classes.
    not_ignore_mask = \
                tf.to_float(tf.equal(scaled_labels, 0))*cfg.weights[0] + \
                tf.to_float(tf.equal(scaled_labels, 1))*cfg.weights[1] + \
                tf.to_float(tf.equal(scaled_labels, 2))*cfg.weights[2] + \
                tf.to_float(tf.equal(scaled_labels, 3))*cfg.weights[3] + \
                tf.to_float(tf.equal(scaled_labels, ignore_label))*cfg.weights[4]

    Add configuration in config_plant.py

5. In train.py
    set params in the deeplab/common.py image_pyramid = [0.25, 0.5, 1]
    set initialize_last_layer = False, last_layers_contain_logits_only=False, train_logdir="checkpoints/"
    Modify hyperparameters
    base_learning_rate=1e-3
    learning_rate_decay_step=500
    learning_rate_decay_factor=0.1
    atrous_rates config modify
    dataset name
    dataset_dir 
    Mofify atrous_rates and output_stride config

    add the slim path in the file.
    find -name "slim"
    ./anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim
    export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim

    Run the command for training -if running from your own checkpoints remove the exclude list from train_utils.py
	- difference between running xcpetion vs mobilenet (atrous rate)
    python deeplab/train.py --model_variant="xception_65" --decoder_output_stride = 4
    /Mobile net??


    Fine tune batch norm is set to False: less training examples, might be better for convergence



    how different atrous_rates have an impact?
    
    Check how weights are initialised 
    change weight initialization acc to some strategy
    2 model changes

